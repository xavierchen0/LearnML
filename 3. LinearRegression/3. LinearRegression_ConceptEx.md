### 1) Describe the null hypotheses to which the p-values given in Table 3.4 correspond. Explain what conclusions you can draw based on these p-values. Your explanation should be phrased in terms of sales, TV, radio, and newspaper, rather than in terms of the coefficients of the linear model.

$$
H_0: \beta_1= \beta_2=\cdots=\beta_p=0\\ 
H_\alpha: \text{at least one of the predictors $\beta_j$ have a non-zero coefficient.}
$$

Because the p-values for TV and radio is low, we can conclude that the relationship between TV and sales and the relationship between radio and sales are statistically significant and we can reject their null hypotheses. On the other hand, the p-value associated with newspaper is high which tells us that the relationship between sales and radio is not statistically significant, and we do not reject the null hypothesis for newspaper.

### 2) Carefully explain the differences between the KNN classifier and KNN regression methods.
KNN classifier and KNN regression methods are closely related in formula. However, the final result of KNN classifier is the classification output for Y (qualitative), where as the output for a KNN regression predicts the quantitative value for f(X).

### 3) Suppose we have a data set with five predictors, $X_1$ = GPA, $X_2$ = IQ, $X_3$ = Level (1 for College and 0 for High School), $X_4$ = Interaction between GPA and IQ, and $X_5$ = Interaction between GPA and Level. The response is starting salary after graduation (in thousands of dollars). Suppose we use least squares to fit the model, and get $\hat{\beta_0}=50, \hat{\beta_1}=20, \hat{\beta_2}=0.07, \hat{\beta_3}=35, \hat{\beta_4}=0.01, \hat{\beta_5}=-10$.

### 3a) Which answer is correct, and why?
- i. For a fixed value of IQ and GPA, high school graduates earn more, on average, than college graduates.
- ii. For a fixed value of IQ and GPA, college graduates earn more, on average, than high school graduates.
- iii. For a fixed value of IQ and GPA, high school graduates earn more, on average, than college graduates provided that the GPA is high enough.
- iv. For a fixed value of IQ and GPA, college graduates earn more, on average, than high school graduates provided that the GPA is high enough.

Ans: iii
$$
\text{Starting Salary} = \hat{\beta_0} + \hat{\beta_1} \times GPA + \hat{\beta_2} \times IQ + \hat{\beta_3} \times Level + \hat{\beta_4} \times GPA \cdot IQ + \hat{\beta_5} \times GPA \cdot Level \\
StartingSalary_{College} = \hat{\beta_0} + \hat{\beta_1} \times GPA + \hat{\beta_2} \times IQ + \hat{\beta_3}+ \hat{\beta_4} \times GPA \cdot IQ + \hat{\beta_5} \times GPA \\
StartingSalary_{High School} = \hat{\beta_0} + \hat{\beta_1} \times GPA + \hat{\beta_2} \times IQ + \hat{\beta_4} \times GPA \cdot IQ\\
StartingSalary_{College} - StartingSalary_{High School} = \hat{\beta_3} + \hat{\beta_5} \times GPA = 35 + (-10) \times GPA
$$
$\hat{\beta_0}$ can be interpreted as the average starting salary of high school students.

### 3b) Predict the salary of a college graduate with IQ of 110 and a GPA of 4.0.
1371.1

### 3c) True or false: Since the coefficient for the GPA/IQ interaction term is very small, there is very little evidence of an interaction effect. Justify your answer.
False. We must examine the p-value of the regression coefficient to determine if the interaction term is statistically significant or not.

### 4 I collect a set of data (n = 100 observations) containing a single predictor and a quantitative response. I then fit a linear regression model to the data, as well as a separate cubic regression, i.e. $Y = \beta_0 + \beta_1X + \beta_2X2 + \beta_3X3 +\epsilon$.

### 4a) Suppose that the true relationship between X and Y is linear, i.e. $Y = \beta_0 + \beta_1X+\epsilon$. Consider the training residual sum of squares (RSS) for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.

For training data, RSS decreases as we increase model flexibility but we run the risk of overfitting our data. Since a cubic regression is more flexible than a linear regression, I would expect the cubic regression to have a lower RSS.

### 4b) Answer (a) using test rather than training RSS.

I would expect the cubic regression to have a higher test RSS as the overfit from training would have more error than the linear regression.

### 4c) Suppose that the true relationship between X and Y is not linear, but we donâ€™t know how far it is from linear. Consider the training RSS for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.

Polynomial regression has lower train RSS than the linear fit because of higher flexibility: no matter what the underlying true relationship is the more flexible model will follow the points more closely and reduce train RSS.

### 4d) Answer (c) using test rather than training RSS.

There is not enough information to tell which test RSS would be lower for either regression given the problem statement is defined as not knowing "how far it is from linear". If it is closer to linear than cubic, the linear regression test RSS could be lower than the cubic regression test RSS. Or, if it is closer to cubic than linear, the cubic regression test RSS could be lower than the linear regression test RSS. It is dues to bias-variance tradeoff: it is not clear what level of flexibility will fit data better.

### 5 Consider the fitted values that result from performing linear regression without an intercept. In this setting, the ith fitted value takes the form

$$\hat{y}_i = x_i \hat{\beta},$$

where

$$\hat{\beta} = \left( \frac{\sum_{i=1}^n x_i y_i}{\sum_{i'=1}^n x_{i'}^2} \right).$$

Show that we can write

$$\hat{y}_i = \sum_{i'=1}^n \alpha'_{i} y_{i'}.$$

What is $\alpha'_i$?


$$
\begin{align*}
y_i &= x_i \hat{\beta} \\
\hat{\beta} &= \frac{\sum_{i=1}^N x_i y_i}{\sum_{i=1}^N x_i^2} \quad \text{(0)} \\
y_i &= x_i \hat{\beta} \\
&= x_i \frac{\sum_{j=1}^N x_j y_j}{\sum_{j=1}^N x_j^2} \\
&= \frac{\sum_{j=1}^N x_i x_j y_j}{\sum_{j=1}^N x_j^2} \\
&= \sum_{i=1}^N \left(\frac{x_i x_j}{\sum_{j=1}^N x_j^2}\right) y_j \\
&= \sum_{i=1}^N a_i y_i \\
\text{where } a_i &= \frac{\sum_{j=1}^N x_i x_j}{\sum_{j=1}^N x_j^2}
\end{align*}
$$

### 6 Using (3.4), argue that in the case of simple linear regression, the least squares line always passes through the point $(\bar{x}, \bar{y})$.

$$
\begin{align*}
\text{The RSS Line is} \\
Y &= \beta_0 + \beta_1 X \\
\text{where} \\
\beta_0 &= \bar{Y} - \beta_1 \bar{X} \quad \text{(1)} \\
\beta_1 &= \frac{\sum_{i=1}^n (x_i - \bar{X})(y_i - \bar{Y})}{\sum_{i=1}^n (x_i - \bar{X})^2} \quad \text{(2)} \\
Y &= \beta_0 + \beta_1 X \\
Y &= \bar{Y} - \beta_1 \bar{X} + \beta_1 X \quad \text{(from (1) and (2))} \\
\text{If } X = \bar{X}, \\
Y &= \bar{Y} - \beta_1 \bar{X} + \beta_1 \bar{X} \\
Y &= \bar{Y} \\
\text{hence } (x_i, y_i) \text{ lies on RSS line.}
\end{align*}
$$

### 7. It is claimed in the text that in the case of simple linear regression of Y onto X, the $R^2$ statistic (3.17) is equal to the square of the correlation between X and Y (3.18). Prove that this is the case. For simplicity, you may assume that $\bar{x} = \bar{y} = 0$.

Generic Solution
--------------------------------------------------------

**Proposition**: Prove that in case of simple linear regression:

$$ y = \beta_0 + \beta_1 x + \varepsilon $$

the $R^2$ is equal to correlation between X and Y squared, e.g.:

$$ R^2 = corr^2(x, y) $$

We'll be using the following definitions to prove the above proposition.

**Def**:
$$R^2 = \frac{TSS - RSS}{TSS}$$

**Def**:
$$TSS = \sum (y_i - \bar{y})^2$$

**Def**:
$$RSS = \sum (y_i - \hat{y}_i)^2$$ 

**Def**:
$$\begin{align}
  corr(x, y) &= \frac{\sum (x_i - \bar{x}) (y_i - \bar{y})}
                     {\sigma_x \sigma_y} \\
  \sigma_x^2 &= \sum (x_i - \bar{x})^2 \\
  \sigma_y^2 &= \sum (y_i - \bar{y})^2
\end{align}$$

**Proof**:

Substitute defintions of TSS and RSS into $R^2$:

$$
R^2 = \frac{\sum (y_i - \bar{y})^2 - \sum (y_i - \hat{y}_i)^2}
           {\sum (y_i - \bar{y})^2}
$$

Let's work on the numerator:

$$
\begin{align}
  A &= \sum (y_i - \bar{y})^2 - \sum (y_i - \hat{y}_i)^2 \\
    &= \sum \left[ (y_i - \bar{y}) - (y_i - \hat{y}_i) \right] 
            \left[ (y_i - \bar{y}) + (y_i - \hat{y}_i) \right] \\
    &= \sum (\hat{y}_i - \bar{y})
            (2y_i - \bar{y} - \hat{y}_i)
\end{align}
$$

Recall that:

$$
\begin{align}
  \hat{\beta}_0 &= \bar{y} - \hat{\beta}_1 \bar{x} \\
  \hat{\beta}_1 &= \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}
                        {\sum (x_j - \bar{x})^2}
\end{align}
$$

Substitute the expression for $\hat{\beta}_0$ into $\hat{y}_i$:

$$
\begin{align}
  \hat{y}_i &= \hat{\beta}_0 + \hat{\beta}_1 x_i \\
            &= \bar{y} - \hat{\beta}_1 \bar{x} + \hat{\beta}_1 x_i \\
            &= \bar{y} + \hat{\beta}_1 (x_i - \bar{x})
\end{align}
$$

Let's analyze two terms from $A$:

$$
\begin{align}
         \hat{y}_i - \bar{y} &= \hat{\beta}_1 (x_i - \bar{x}) \\
  2y_i - \bar{y} - \hat{y}_i &= 2y_i - \bar{y} - \bar{y} -
                                \hat{\beta}_1 (x_i - \bar{x}) \\
                             &= 2(y_i - \bar{y}) - 
                                \hat{\beta}_1 (x_i - \bar{x}) 
\end{align}
$$

and substitute these expressions back into $A$:

$$
\begin{align}
  A &= \sum \hat{\beta}_1 (x_i - \hat{x})
            \left[ 2(y_i - \bar{y}) - \hat{\beta}_1 (x_i - \bar{x}) \right] \\
    &= \hat{\beta}_1 \sum (x_i - \bar{x})
                          \left[ 2(y_i - \bar{y}) -
                                 \hat{\beta}_1 (x_i - \bar{x}) \right] \\
    &= \hat{\beta}_1
       \left[ 2 \sum (x_i - \bar{x})(y_i - \bar{y}) -
              \hat{\beta}_1 \sum (x_i - \bar{x})^2  \right]
\end{align}
$$

Using formula for $\hat{\beta}_1$ it is easy to see that the last term is
nothing but:

$$ \sum (x_i - \bar{x}) (y_i - \bar{y}) $$

Thus, we get:

$$
\begin{align}
  A &= \hat{\beta}_1 \sum (x_i - \bar{x}) (y_i - \bar{y}) \\
    &= \frac{\left[ \sum (x_i - \bar{x}) (y_i - \bar{y}) \right]^2}
            {\sum (x_j - \bar{x})^2}
\end{align}
$$

Plug the final expression for $A$ back into $R^2$:

$$
R^2 = \frac{\left[ \sum (x_i - \bar{x}) (y_i - \bar{y}) \right]^2}
           {\sum (x_j - \bar{x})^2 \sum (y_k - \bar{y})^2}
$$

Compare this to the definition of correlation and get:

$$ R^2 = corr^2(x, y) $$