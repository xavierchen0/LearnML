# Question 1
> Using basic statistical properties of the variance, as well as single- variable calculus, derive (5.6). In other words, prove that α given by (5.6) does indeed minimize Var(αX + (1 − α)Y ).
$$\alpha = \frac {\sigma_Y^2 - \sigma_{XY}}
               {\sigma_X^2 + \sigma_Y^2 - 2 \sigma_{XY}}
$$

Using the following rules:

$$
Var(X+Y) = Var(X) + Var(Y) + 2 Cov(X,Y)
\\
Var(cX) = c^2 Var(X)
\\
Cov(cX,Y) = Cov(X,cY) = c Cov(X,Y)
$$

Minimizing two-asset financial portfolio:
$$
Var(\alpha X + (1 - \alpha)Y)
\\
= Var(\alpha X) + Var((1 - \alpha) Y) + 2 Cov(\alpha X, (1 - \alpha) Y)
\\
= \alpha^2 Var(X) + (1 - \alpha)^2 Var(Y) + 2 \alpha (1 - \alpha) Cov(X, Y)
\\
= \sigma_X^2 \alpha^2 + \sigma_Y^2 (1 - \alpha)^2 + 2 \sigma_{XY} (-\alpha^2 +
\alpha)
$$

Take the first derivative to find critical points:
$$
0 = \frac {d} {d\alpha} f(\alpha)
\\
0 = 2 \sigma_X^2 \alpha + 2 \sigma_Y^2 (1 - \alpha) (-1) + 2 \sigma_{XY}
(-2 \alpha + 1)
\\
0 = \sigma_X^2 \alpha + \sigma_Y^2 (\alpha - 1) + \sigma_{XY} (-2 \alpha + 1)
\\
0 = (\sigma_X^2 + \sigma_Y^2 - 2 \sigma_{XY}) \alpha - \sigma_Y^2 + \sigma_{XY}
\\
\alpha = \frac {\sigma_Y^2 - \sigma_{XY}}
               {\sigma_X^2 + \sigma_Y^2 - 2 \sigma_{XY}}
$$

# Question 2
> We will now derive the probability that a given observation is part of a bootstrap sample. Suppose that we obtain a bootstrap sample from a set of n observations.

> (a) What is the probability that the first bootstrap observation is not the jth observation from the original sample? Justify your answer.

$1 - 1/n$

> (b) What is the probability that the second bootstrap observation is not the jth observation from the original sample?

Since each bootstrap observation is a random sample, this probability is the same ($1 - 1/n$).

> (c) Argue that the probability that the $j$th observation is not in the bootstrap sample is $(1 − 1/n)$.

In bootstrap, we sample with replacement so each observation in the bootstrap
sample has the same 1/n (independent) chance of equaling the jth observation. 
Applying the product rule for a total of n observations gives us $(1 - 1/n)^n$.

> (d) When n = 5, what is the probability that the jth observation is in the bootstrap sample?

$Pr(in) = 1 - Pr(out) = 1 - (1 - 1/5)^5 = 1 - (4/5)^5 = 67.2\%$

> (e) When n = 100, what is the probability that the jth observation is in the bootstrap sample?

$Pr(in) = 1 - Pr(out) = 1 - (1 - 1/100)^{100} = 1 - (99/100)^{100} = 63.4\%$

> (f) When n = 10, 000, what is the probability that the jth observation is in the bootstrap sample?

$1 - (1 - 1/10000)^{10000} = 63.2\%$

# Question 3
> We now review k-fold cross-validation.

> (a) Explain how k-fold cross-validation is implemented.

k-fold cross-validation is implemented by taking the set of n observations and
randomly splitting into k non-overlapping groups. Each of these groups acts as
a validation set and the remainder as a training set. The test error is
estimated by averaging the k resulting MSE estimates.

> (b) What are the advantages and disadvantages of k-fold cross-
validation relative to:

> i. The validation set approach?

The advantage of K-fold cross-validation is that the model introduces much less bias than the validation set approach because it is trained on more observations, and tend not to overestimate the test error rate. The disadvantage is that it is more computationally intensive because the model needs to be refitted K-times, whereas in the validation set approach, the model is only fitted once.

> ii. LOOCV?

The advantage of K-fold cross-validation over LOOCV is that it is less computationally intensive because the model is only fitted K times where K < N. Moreover, K-fold cross-validation tends to provide better estimate of the test error rate because the models are trained on less correlated observations, and thus have a lower variance of the estimated mean. The disadvantage is that K-fold cross-validation has a higher bias due to training on less observations than LOOCV/

# Question 4
> Suppose that we use some statistical learning method to make a prediction for the response Y for a particular value of the predictor X. Carefully describe how we might estimate the standard deviation of our prediction.

We can use the bootstrap approach. The
bootstrap approach works by repeatedly sampling observations (with replacement)
from the original data set $B$ times, for some large value of $B$, each time
fitting a new model and subsequently obtaining the RMSE of the estimates for all
$B$ models.